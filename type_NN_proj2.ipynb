{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnM1eHFxWszGk9pgMCaIz1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abel-kaiki/Modelling/blob/main/type_NN_proj2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjXACuqFFh_p",
        "outputId": "1cc335be-a0ba-4fea-ebd0-094c8a0bf17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import libraries used in this project\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "#Colab drive mount\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#Load dataset\n",
        "path = '/content/drive/MyDrive/ColabNotebooks/Datasets/R_W_Wine_quality.csv'\n",
        "data = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing dataframe - Splitting\n",
        "# Separate inputs and outputs\n",
        "inputs = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "data['Id'] = data.index\n",
        "X = data[['Id'] + inputs] # include Id for tracking\n",
        "\n",
        "#Need to convert 'type' to numerical (float OR binary)\n",
        "data['type_int'] = (data['type'] == 'Red') #Red = True (1), White = False (0)\n",
        "data['type_int'] = data['type_int'].astype(float)\n",
        "data['quality'] = data['quality'].astype(float)\n",
        "\n",
        "y = data[['quality','type_int']]\n",
        "quality_min = min(data['quality'])\n",
        "quality_max = max(data['quality'])\n",
        "quality_range = quality_max - quality_min + 1 #Including min value\n",
        "\n",
        "# Split data into training and testing sets, retaining IDs.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# test_size : testing-training splits. 0.2 = 20% reserved for testing."
      ],
      "metadata": {
        "id": "vErmllETF4G7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing df - Scaling\n",
        "# fit and transform, scale between 1 and 0\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[inputs])\n",
        "X_test_scaled = scaler.fit_transform(X_test[inputs])\n",
        "num_classes = len(y.columns)\n",
        "\n",
        "y_qual = y['quality'].values.reshape(-1, 1)\n",
        "y_obj = scaler.fit(y_qual)\n",
        "\n",
        "# y scaling (quality)\n",
        "#y_train_scaled = scaler.transform(y_train)\n",
        "#y_test_scaled = MinMaxScaler().transform(y_test)\n",
        "\n",
        "#as dataframe\n",
        "#y_train_scaled = pd.DataFrame(y_train_scaled, columns=['quality', 'type'])\n",
        "#y_test_scaled = pd.DataFrame(y_test_scaled, columns=['quality', 'type'])\n",
        "\n",
        "#y_train_scaled_nd = MinMaxScaler().fit_transform(y_train)\n",
        "#y_test_scaled_nd = MinMaxScaler().fit_transform(y_test)\n",
        "\n",
        "#num_classes"
      ],
      "metadata": {
        "id": "Z_whZm-HF7NY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build model\n",
        "\n",
        "#Sequential model cannot do multi-output, See: https://keras.io/guides/functional_api/#models-with-multiple-inputs-and-outputs\n",
        "# In our case we need a softmax output for the quality, and a sigmoid output for the binary\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(6, activation='relu', input_shape=(len(inputs),)), #First hidden layer with 64 neurons, relu activation function\n",
        "    Dense(3, activation='relu'), # second hidden layer with 32 neurons\n",
        "    Dense(2, activation='sigmoid') #output layer for classification (softmax for multiclass, sigmoid for binary)\n",
        "])\n",
        "\n",
        "# Initialise layers\n",
        "#input_layer = Input(shape=(len(inputs),))\n",
        "#hidden = Dense(6, activation='relu')(input_layer)\n",
        "#hidden = Dense(16, activation='relu')(hidden)\n",
        "#output1 = Dense(1, activation='sigmoid')(hidden)\n",
        "#output2 = Dense(1, activation='sigmoid')(hidden)\n",
        "\n",
        "##\n",
        "#model = Model(inputs=input_layer, outputs=[output1, output2]) #multi-output\n",
        "#model = Sequential(inputs=input_layer, outputs=output1)\n",
        "\n",
        "#Compiling\n",
        "#sgd=SGD(lr=0.9) use this if you want to change lr in SGD\n",
        "#Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) use this if you want to change lr in Adam, default lr = 0.001\n",
        "opti = Adam(learning_rate= 0.0001)\n",
        "#model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy','binary_crossentropy']) #multi-output\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.compile(optimizer = opti, loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "x0LE2gcGH2fR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting model\n",
        "model.fit(X_train_scaled, y_train['type_int'], validation_split=0.2, batch_size = 15, epochs = 200)\n",
        "#[print(i.shape, i.dtype) for i in model.inputs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHbrKJCIB-y",
        "outputId": "c85f3720-6fff-421d-b529-7e4f60331b09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "278/278 [==============================] - 3s 4ms/step - loss: 0.6760 - val_loss: 0.6688\n",
            "Epoch 2/200\n",
            "278/278 [==============================] - 3s 10ms/step - loss: 0.6596 - val_loss: 0.6518\n",
            "Epoch 3/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.6419 - val_loss: 0.6327\n",
            "Epoch 4/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.6227 - val_loss: 0.6138\n",
            "Epoch 5/200\n",
            "278/278 [==============================] - 3s 9ms/step - loss: 0.6037 - val_loss: 0.5947\n",
            "Epoch 6/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.5842 - val_loss: 0.5752\n",
            "Epoch 7/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5644 - val_loss: 0.5555\n",
            "Epoch 8/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.5443 - val_loss: 0.5355\n",
            "Epoch 9/200\n",
            "278/278 [==============================] - 3s 12ms/step - loss: 0.5243 - val_loss: 0.5157\n",
            "Epoch 10/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.5043 - val_loss: 0.4959\n",
            "Epoch 11/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.4845 - val_loss: 0.4764\n",
            "Epoch 12/200\n",
            "278/278 [==============================] - 2s 6ms/step - loss: 0.4649 - val_loss: 0.4573\n",
            "Epoch 13/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.4459 - val_loss: 0.4388\n",
            "Epoch 14/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.4274 - val_loss: 0.4208\n",
            "Epoch 15/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.4096 - val_loss: 0.4033\n",
            "Epoch 16/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3926 - val_loss: 0.3868\n",
            "Epoch 17/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.3707\n",
            "Epoch 18/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3554\n",
            "Epoch 19/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3456 - val_loss: 0.3407\n",
            "Epoch 20/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.3314 - val_loss: 0.3269\n",
            "Epoch 21/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3180 - val_loss: 0.3138\n",
            "Epoch 22/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.3051 - val_loss: 0.3009\n",
            "Epoch 23/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.2930 - val_loss: 0.2890\n",
            "Epoch 24/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.2815 - val_loss: 0.2776\n",
            "Epoch 25/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2705 - val_loss: 0.2669\n",
            "Epoch 26/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2601 - val_loss: 0.2565\n",
            "Epoch 27/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2501 - val_loss: 0.2466\n",
            "Epoch 28/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2406 - val_loss: 0.2372\n",
            "Epoch 29/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2316 - val_loss: 0.2283\n",
            "Epoch 30/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2230 - val_loss: 0.2198\n",
            "Epoch 31/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2148 - val_loss: 0.2116\n",
            "Epoch 32/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.2070 - val_loss: 0.2038\n",
            "Epoch 33/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1995 - val_loss: 0.1964\n",
            "Epoch 34/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1924 - val_loss: 0.1893\n",
            "Epoch 35/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1856 - val_loss: 0.1825\n",
            "Epoch 36/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1791 - val_loss: 0.1761\n",
            "Epoch 37/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1729 - val_loss: 0.1699\n",
            "Epoch 38/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1670 - val_loss: 0.1638\n",
            "Epoch 39/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1613 - val_loss: 0.1582\n",
            "Epoch 40/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1558 - val_loss: 0.1527\n",
            "Epoch 41/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1507 - val_loss: 0.1475\n",
            "Epoch 42/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.1457 - val_loss: 0.1425\n",
            "Epoch 43/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.1409 - val_loss: 0.1378\n",
            "Epoch 44/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.1363 - val_loss: 0.1332\n",
            "Epoch 45/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.1319 - val_loss: 0.1288\n",
            "Epoch 46/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.1278 - val_loss: 0.1246\n",
            "Epoch 47/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1238 - val_loss: 0.1206\n",
            "Epoch 48/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1200 - val_loss: 0.1168\n",
            "Epoch 49/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1165 - val_loss: 0.1131\n",
            "Epoch 50/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1130 - val_loss: 0.1097\n",
            "Epoch 51/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1096 - val_loss: 0.1064\n",
            "Epoch 52/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1065 - val_loss: 0.1032\n",
            "Epoch 53/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1035 - val_loss: 0.1002\n",
            "Epoch 54/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.1006 - val_loss: 0.0973\n",
            "Epoch 55/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0978 - val_loss: 0.0945\n",
            "Epoch 56/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0952 - val_loss: 0.0919\n",
            "Epoch 57/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0926 - val_loss: 0.0894\n",
            "Epoch 58/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0901 - val_loss: 0.0870\n",
            "Epoch 59/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0878 - val_loss: 0.0847\n",
            "Epoch 60/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0856 - val_loss: 0.0825\n",
            "Epoch 61/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0835 - val_loss: 0.0804\n",
            "Epoch 62/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0814 - val_loss: 0.0785\n",
            "Epoch 63/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0796 - val_loss: 0.0765\n",
            "Epoch 64/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0777 - val_loss: 0.0747\n",
            "Epoch 65/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0759 - val_loss: 0.0730\n",
            "Epoch 66/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0742 - val_loss: 0.0713\n",
            "Epoch 67/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0726 - val_loss: 0.0697\n",
            "Epoch 68/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0711 - val_loss: 0.0682\n",
            "Epoch 69/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0696 - val_loss: 0.0668\n",
            "Epoch 70/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0682 - val_loss: 0.0654\n",
            "Epoch 71/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0669 - val_loss: 0.0641\n",
            "Epoch 72/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0656 - val_loss: 0.0628\n",
            "Epoch 73/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0644 - val_loss: 0.0616\n",
            "Epoch 74/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0631 - val_loss: 0.0604\n",
            "Epoch 75/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0620 - val_loss: 0.0593\n",
            "Epoch 76/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0610 - val_loss: 0.0584\n",
            "Epoch 77/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0600 - val_loss: 0.0574\n",
            "Epoch 78/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0590 - val_loss: 0.0564\n",
            "Epoch 79/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0580 - val_loss: 0.0556\n",
            "Epoch 80/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0572 - val_loss: 0.0547\n",
            "Epoch 81/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0564 - val_loss: 0.0539\n",
            "Epoch 82/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0555 - val_loss: 0.0531\n",
            "Epoch 83/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0546 - val_loss: 0.0523\n",
            "Epoch 84/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0539 - val_loss: 0.0516\n",
            "Epoch 85/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0531 - val_loss: 0.0509\n",
            "Epoch 86/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0524 - val_loss: 0.0502\n",
            "Epoch 87/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0517 - val_loss: 0.0495\n",
            "Epoch 88/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0510 - val_loss: 0.0490\n",
            "Epoch 89/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0504 - val_loss: 0.0483\n",
            "Epoch 90/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0498 - val_loss: 0.0477\n",
            "Epoch 91/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0492 - val_loss: 0.0472\n",
            "Epoch 92/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0486 - val_loss: 0.0467\n",
            "Epoch 93/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0480 - val_loss: 0.0461\n",
            "Epoch 94/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0475 - val_loss: 0.0457\n",
            "Epoch 95/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0470 - val_loss: 0.0452\n",
            "Epoch 96/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0465 - val_loss: 0.0448\n",
            "Epoch 97/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0460 - val_loss: 0.0443\n",
            "Epoch 98/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0455 - val_loss: 0.0439\n",
            "Epoch 99/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0450 - val_loss: 0.0435\n",
            "Epoch 100/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0446 - val_loss: 0.0431\n",
            "Epoch 101/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0442 - val_loss: 0.0427\n",
            "Epoch 102/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0437 - val_loss: 0.0424\n",
            "Epoch 103/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0434 - val_loss: 0.0420\n",
            "Epoch 104/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0430 - val_loss: 0.0417\n",
            "Epoch 105/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0426 - val_loss: 0.0414\n",
            "Epoch 106/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0423 - val_loss: 0.0411\n",
            "Epoch 107/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0419 - val_loss: 0.0408\n",
            "Epoch 108/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0416 - val_loss: 0.0405\n",
            "Epoch 109/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0413 - val_loss: 0.0402\n",
            "Epoch 110/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0410 - val_loss: 0.0399\n",
            "Epoch 111/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0406 - val_loss: 0.0397\n",
            "Epoch 112/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0404 - val_loss: 0.0395\n",
            "Epoch 113/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0401 - val_loss: 0.0392\n",
            "Epoch 114/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0398 - val_loss: 0.0390\n",
            "Epoch 115/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0396 - val_loss: 0.0388\n",
            "Epoch 116/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0393 - val_loss: 0.0385\n",
            "Epoch 117/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0390 - val_loss: 0.0383\n",
            "Epoch 118/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0388 - val_loss: 0.0381\n",
            "Epoch 119/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0385 - val_loss: 0.0379\n",
            "Epoch 120/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0383 - val_loss: 0.0377\n",
            "Epoch 121/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0380 - val_loss: 0.0376\n",
            "Epoch 122/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0378 - val_loss: 0.0374\n",
            "Epoch 123/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0376 - val_loss: 0.0372\n",
            "Epoch 124/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0374 - val_loss: 0.0371\n",
            "Epoch 125/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0372 - val_loss: 0.0369\n",
            "Epoch 126/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0370 - val_loss: 0.0367\n",
            "Epoch 127/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0368 - val_loss: 0.0366\n",
            "Epoch 128/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0366 - val_loss: 0.0364\n",
            "Epoch 129/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0364 - val_loss: 0.0363\n",
            "Epoch 130/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0362 - val_loss: 0.0362\n",
            "Epoch 131/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0361 - val_loss: 0.0361\n",
            "Epoch 132/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0359 - val_loss: 0.0359\n",
            "Epoch 133/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0357 - val_loss: 0.0358\n",
            "Epoch 134/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0356 - val_loss: 0.0357\n",
            "Epoch 135/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0354 - val_loss: 0.0356\n",
            "Epoch 136/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0352 - val_loss: 0.0354\n",
            "Epoch 137/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0351 - val_loss: 0.0353\n",
            "Epoch 138/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0349 - val_loss: 0.0352\n",
            "Epoch 139/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0348 - val_loss: 0.0351\n",
            "Epoch 140/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0347 - val_loss: 0.0350\n",
            "Epoch 141/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0345 - val_loss: 0.0349\n",
            "Epoch 142/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0344 - val_loss: 0.0348\n",
            "Epoch 143/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0342 - val_loss: 0.0347\n",
            "Epoch 144/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0341 - val_loss: 0.0346\n",
            "Epoch 145/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0339 - val_loss: 0.0345\n",
            "Epoch 146/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0339 - val_loss: 0.0344\n",
            "Epoch 147/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0337 - val_loss: 0.0344\n",
            "Epoch 148/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0335 - val_loss: 0.0343\n",
            "Epoch 149/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0335 - val_loss: 0.0342\n",
            "Epoch 150/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0333 - val_loss: 0.0341\n",
            "Epoch 151/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0332 - val_loss: 0.0341\n",
            "Epoch 152/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0331 - val_loss: 0.0340\n",
            "Epoch 153/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0330 - val_loss: 0.0339\n",
            "Epoch 154/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0329 - val_loss: 0.0339\n",
            "Epoch 155/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0328 - val_loss: 0.0338\n",
            "Epoch 156/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0327 - val_loss: 0.0337\n",
            "Epoch 157/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0326 - val_loss: 0.0337\n",
            "Epoch 158/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0325 - val_loss: 0.0336\n",
            "Epoch 159/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0324 - val_loss: 0.0336\n",
            "Epoch 160/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0323 - val_loss: 0.0335\n",
            "Epoch 161/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0322 - val_loss: 0.0335\n",
            "Epoch 162/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0321 - val_loss: 0.0334\n",
            "Epoch 163/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0320 - val_loss: 0.0334\n",
            "Epoch 164/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0318 - val_loss: 0.0334\n",
            "Epoch 165/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0318 - val_loss: 0.0333\n",
            "Epoch 166/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0317 - val_loss: 0.0332\n",
            "Epoch 167/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0316 - val_loss: 0.0332\n",
            "Epoch 168/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0315 - val_loss: 0.0332\n",
            "Epoch 169/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0314 - val_loss: 0.0331\n",
            "Epoch 170/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0313 - val_loss: 0.0331\n",
            "Epoch 171/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0312 - val_loss: 0.0331\n",
            "Epoch 172/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0311 - val_loss: 0.0330\n",
            "Epoch 173/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0310 - val_loss: 0.0329\n",
            "Epoch 174/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0309 - val_loss: 0.0328\n",
            "Epoch 175/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0308 - val_loss: 0.0328\n",
            "Epoch 176/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0307 - val_loss: 0.0327\n",
            "Epoch 177/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0306 - val_loss: 0.0326\n",
            "Epoch 178/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0305 - val_loss: 0.0326\n",
            "Epoch 179/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0303 - val_loss: 0.0325\n",
            "Epoch 180/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0303 - val_loss: 0.0325\n",
            "Epoch 181/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0301 - val_loss: 0.0324\n",
            "Epoch 182/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0300 - val_loss: 0.0323\n",
            "Epoch 183/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0299 - val_loss: 0.0322\n",
            "Epoch 184/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0298 - val_loss: 0.0322\n",
            "Epoch 185/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0296 - val_loss: 0.0322\n",
            "Epoch 186/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0295 - val_loss: 0.0322\n",
            "Epoch 187/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0295 - val_loss: 0.0322\n",
            "Epoch 188/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0294 - val_loss: 0.0322\n",
            "Epoch 189/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0293 - val_loss: 0.0322\n",
            "Epoch 190/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0292 - val_loss: 0.0322\n",
            "Epoch 191/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0292 - val_loss: 0.0321\n",
            "Epoch 192/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0292 - val_loss: 0.0322\n",
            "Epoch 193/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0290 - val_loss: 0.0322\n",
            "Epoch 194/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0290 - val_loss: 0.0322\n",
            "Epoch 195/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0289 - val_loss: 0.0322\n",
            "Epoch 196/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0288 - val_loss: 0.0322\n",
            "Epoch 197/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0288 - val_loss: 0.0322\n",
            "Epoch 198/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0287 - val_loss: 0.0322\n",
            "Epoch 199/200\n",
            "278/278 [==============================] - 1s 3ms/step - loss: 0.0287 - val_loss: 0.0322\n",
            "Epoch 200/200\n",
            "278/278 [==============================] - 1s 2ms/step - loss: 0.0286 - val_loss: 0.0322\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x784f9f76ab30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract losses for training datasets\n",
        "training_loss = model.history.history['loss']\n",
        "epochs = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
        "plt.title('Training loss/epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "XC34w8T5IIjr",
        "outputId": "72d11066-7f19-4062-da74-151f507942ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model - type\n",
        "#Use trained model to make predictions based on scaled test data\n",
        "predictions = model.predict(X_test_scaled)\n",
        "preds_type = predictions.argmax(axis=1)\n",
        "#y_obj = MinMaxScaler().fit(y_test_scaled_nd[0].reshape(-1,1))\n",
        "#predicted_quality = y_obj.inverse_transform(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzzUR90oILVo",
        "outputId": "fd003611-47ae-4fed-e7c8-20c666a78d47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 1s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transform numerical class indices back to original labels\n",
        "#predicted_labels = LabelEncoder.inverse_transform(predicted_quality)\n",
        "\n",
        "#Actual quality for test data\n",
        "actual_type = y_test['type_int'].values\n",
        "\n",
        "#Calculate accuracy\n",
        "correct_predictions = (preds_type == actual_type).sum()\n",
        "accuracy = correct_predictions / len(actual_type)\n",
        "print(accuracy*100, '% accurate')\n",
        "\n",
        "\n",
        "#pred_range = preds - actual_quality\n",
        "#rmin = min(pred_range)\n",
        "#rmax = max(pred_range)\n",
        "\n",
        "#in_range = 100*((pred_range == 1).sum() + (pred_range == -1).sum())/(len(pred_range) - (pred_range == 0).sum())\n",
        "#in_range2 = (pred_range == 0).sum()/((pred_range == -1).sum() + (pred_range == 1).sum())\n",
        "#print(in_range, '% of erroneous predicted qualities are +-1 of actual quality')\n",
        "#print(in_range2, ': ratio between correct/incorrect by +-1')\n",
        "#print(min(training_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X3U2_FoIOoo",
        "outputId": "7e50dd2b-ecf0-45b0-f6c9-221379816ba9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.38461538461539 % accurate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klhoXT9uIQQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}